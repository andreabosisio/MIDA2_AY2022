%!TEX root = ../main.tex

%TODO

We'll see two approaches
\begin{itemize}
    \item Using Kalman Filter
    \item Using Simulation error method
\end{itemize}

\section{Using Kalman Filter}
Kalman Filter is not a system identification method, it is a variable estimation approach (software-sensor, observer).
However we can use it also for gray-box system identification (\emph{side benefit} of K.F.).

\paragraph{Problem definition} We have a model, typically built as a white-box model using first principles:
\[
    S: \begin{cases}
        x(t+1) = f(x(t), u(t), \theta) + v_1(t) \\
        y(t) = h(x(t), \theta) + v_2(t)
    \end{cases}
\]

$f$ and $h$ are linear or non-linear functions, depending on some unknown parameter $\theta$ (with a physical meaning, e.g. mass, resistance, friction, \dots).

The problem is to estimate $\hat{\theta}$.

K.F. solves this problem by transforming the unknown parameters in extended states: K.F. makes the simultaneous estimation of $\hat{x}(t|t)$ (classic Kalman Filter problem) and $\hat{\theta}(t)$ (parameter identification problem).

\paragraph{Trick} State extension

\[
    S: \begin{cases}
        x(t+1) = f(x(t), u(t), \theta(t)) + v_1(t) \\
        \theta(t+1) = \theta(t) + v_\theta(t) \\
        y(t) = h(x(t), \theta(t)) + v_2(t)
    \end{cases}
\]

The new extended state vector is $x_E = \begin{bmatrix} x(t) \\ \theta(t) \end{bmatrix}$.
The unknown parameters are transformed in unknown variables.

The new equation we created
\[
    \theta(t+1) = \theta(t) + v_\theta(t)
\]
It is a \emph{fictitious} equation (not a physical equation).

The core dynamics is $\theta(t+1)=\theta(t)$, it's the equations of something which is constant.
This is exactly the nature of $\theta(t)$ which is indeed a constant vector of parameters.

We need a \emph{fictitious} noise in order to force Kalman Filter to find the right value of $\theta$ (if no noise in this equation K.F. probably would stay fixed on the initial condition).
We tell K.F. to do not rely on initial conditions.

Notice that this equation is not of an asymptotic stable system but a simply-stable system.
It's not a problem because K.F. can deal with non-asymptotically stable systems.

\paragraph{Design choice} The choice of the covariance matrix of $v_\theta(t) \sim WN(0, V_\theta)$.

We make the empirical assumption that $v_1 \perp v_\theta$ and $v_2 \perp v_\theta$ (there is no reason for $v_\theta$ to be correlated with $v_1$ and $v_2$).
\[
    V_\theta = \begin{bmatrix}
        \lambda_{1\theta}^2 & & & \\
        & \lambda_{2\theta}^2 & & \\
        & & \ddots & \\
        & & & \lambda_{n_\theta\theta}^2\\
    \end{bmatrix}
\]

It is a $n_\theta\times n_\theta$ and usually it is assumed that $\lambda_{1\theta}^2=\lambda_{2\theta}^2=\dots=\lambda_{n_\theta\theta}^2$.
We assume that $v_\theta(t)$ is a set of independent W.N. all with the same variance $\lambda_\theta^2$ (tuned empirically).

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
            node distance=2cm,auto,>=latex',
            declare function={
                f1(\x) = (\x < 2) * (\x/2*(3-1)) +
                         (\x >= 2) * (3-1) +
                         (\x > 0.2) * rand/2.5 +
                         1;
                f2(\x) = (\x < 4.5) * (\x/4.5*(3-1)) +
                         (\x >= 4.5) * (3-1) +
                         (\x > 0.2) * rand/15 +
                         1;
            }
        ]
        \draw[->] (0,0) -- (6,0) node[below] {$t$};
        \draw[->] (0,0) -- (0,4) node[left] {$\theta$};

        \node[green] at (2.3,1.5) {\footnotesize small $\lambda_\theta^2$};
        \node[blue] at (2.3,3.7) {\footnotesize big $\lambda_\theta^2$};

        \draw[dotted] (6,3) -- (0,3) node[left] {$\overline{\theta}$};
        \draw[domain=0:5.5,smooth,variable=\x,blue,samples=70] plot ({\x},{f1(\x)});
        \draw[domain=0:5.5,smooth,variable=\x,green,samples=70] plot ({\x},{f2(\x)});

        \draw[mark=*] plot coordinates {(0,1)} node[left, align=right] {Initial\\condition};
    \end{tikzpicture}
    \caption*{Influence of choice of $\lambda_\theta^2$}
\end{figure}

With a small value of $\lambda_\theta^2$ there is a slow convergence with small oscillations (noise) at steady-state (big trust to initial conditions).
With large values of $\lambda_\theta^2$ there's fast convergence but noisy at steady-state.

$\lambda_\theta^2$ is selected according to the best compromise for your specific application.

\paragraph{Notice} This trick can work in principle with any number of unknown parameters (e.g. 3 sensors, 10 states and 20 parameters).
In practice it works well only on a limited number of parameters (e.g. 3 sensors, 5 states and 2 parameters).

\begin{example}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=2cm,auto,>=latex']
            \draw (0,0) -- (6,0);
            \draw[pattern=north east lines] (0,0) rectangle (-0.5,2);
            \draw[] (2,0) rectangle (4,1.5);
            \node at (3,0.75) {$M$};
            \draw[decoration={aspect=0.3, segment length=1mm, amplitude=2mm,coil},decorate] (0.5,0.75) -- (1.5,0.75);
            \draw (0,0.75) -- (0.5,0.75);
            \draw (1.5,0.75) -- (2,0.75);
            \draw[->] (4,0.75) -- (5,0.75) node[right] {$F$};
            \fill[pattern=north east lines] (1.8,0) rectangle ++(2.4,-0.1) node[above right] {\footnotesize friction $c$};
            \draw[->] (3,-0.5) -- (4,-0.5) node[right] {$x$};
            \draw (3,-0.55) -- (3,-0.45);
        \end{tikzpicture}
    \end{figure}

    \begin{description}
        \item[Input] $F(t)$
        \item[Output] Position $x(t)$ (measured)
        \item[Parameters] $K$ and $M$ are known (measured), $c$ is unknown
    \end{description}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=2cm,auto,>=latex']
            \node[block,align=center] at (0,0) (s) {system\\$c = ?$};
            \draw[<-] (s) -- ++(-1.5,0) node[left] {$F(t)$};
            \draw[->] (s) -- ++(1.5,0) node[right] {$x(t)$};
        \end{tikzpicture}
    \end{figure}

    \paragraph{Problem} Estimate $c$ with a K.F.

    Using K.F. we do not need a training dataset.

    \paragraph{Step 1} Model the system
    \[
        \ddot{x}M = -Kx - c\dot{x} + F
    \]
    It's a differential, continuous time linear equation.
    It's second order so we need 2 state variables: $x_1(t) = x(t)$ and $x_2(t) = \dot{x}(t)$.
    \[
        \begin{cases}
            \dot{x}(t) = x_2(t) \\
            M\dot{x}_2(t) = -Kx_1(t) -cx_2(t) + F(t) \\
            y(t) = x_1(t)
        \end{cases}
        \begin{cases}
            \dot{x}_1(t) = x_2(t) \\
            \dot{x}_2(t) = -\frac{K}{M} x_1(t) - \frac{c}{M} x_2(t) + \frac{1}{M}F(t) \\
            y(t) = x_1(t)
        \end{cases}
    \]

    \paragraph{Step 2} Discretization

    Eulero forward: $\dot{x}(t) \approx \frac{x(t+1)-x(t)}{\Delta}$.

    \[
        \begin{cases}
            \frac{x_1(t+1)-x_1(t)}{\Delta} &= x_2(t) \\
            \frac{x_2(t+1)-x_2(t)}{\Delta} &= -\frac{K}{M} x_1(t) - \frac{c}{M} x_2(t) + \frac{1}{M}F(t) \\
            y(t) &= x_1(t)
        \end{cases}
    \]
    \[
        \begin{cases}
            x_1(t+1) = x_1(t) + \Delta x_2(t) \\
            x_2(t+1) = -\frac{K\Delta}{M}x_1(t) + \left(1-\frac{c\Delta}{M}\right)x_2(t) + \frac{\Delta}{M}F(t) \\
            y(t) = x_1(t)
        \end{cases}
    \]

    \paragraph{Step 3} State extension
    \[
        x_3(t+1) = x_3(t)
    \]
    \[
        \begin{cases}
            x_1(t+1) = x_1(t) + \Delta x_2(t) + v_{11}(t) \\
            x_2(t+1) = -\frac{K\Delta}{M}x_1(t) + \left(1-\frac{\Delta {\color{red}x_3(t)}}{M}\right)x_2(t) + \frac{\Delta}{M}F(t) + v_{12}(t) \\
            {\color{red}x_3(t+1) = x_3(t) + v_{13}(t)} \\
            y(t) = x_1(t) + v_2(t)
        \end{cases}
    \]

    The system is ready for K.F. application: we get at the same time $\hat{x}(t)$ and $\hat{c}(t)$.

    Notice that we need Extended Kalman Filter: even if the original system was linear, state extension moved to a non-linear system.
\end{example}


\section{Using Simulation Error Method}

Are there alternative ways to solve gray-box system identification problems?
A commonly (and intuitive) used method is parametric identification approach based on Simulation Error Method (SEM).

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm,auto,>=latex']
        \node[block, align=center] at (0,0) (sys) {Model with\\some unknown\\parameters};
        \draw[<-] (sys) -- ++(-2,0) node[left] {$u(t)$};
        \draw[->] (sys) -- ++(2,0) node[right] {$y(t)$};
    \end{tikzpicture}
\end{figure}

\paragraph{Step 1} Collect data from an experiment

\begin{align*}
    \{ \tilde{u}(1), \tilde{u}(2), \dots, \tilde{u}(N) \} \\
    \{ \tilde{y}(1), \tilde{y}(2), \dots, \tilde{y}(N) \}
\end{align*}

\paragraph{Step 2} Define model structure
\[
    y(t) = \mathcal{M}(u(t), \overline{\theta}, \theta)
\]
Mathematical model (linear or non-linear) usually written from first principle equations. $\overline{\theta}$ is the set of known parameters (mass, resistance, \dots), $\theta$ is the set of unknown parameters (possibly with bounds).

\paragraph{Step 3} Performance index definition
\[
    J_N(\theta) = \frac{1}{N} \sum_{t=1}^N \left( \tilde{y}(t) - \mathcal{M}(\tilde{u}(t), \overline{\theta}, \theta) \right)^2
\]

\paragraph{Step 4} Optimization

\[
    \hat{\theta}_N = \argmin_\theta J_N(\theta)
\]

\begin{itemize}
    \item Usually no analytic expression of $J_N(\theta)$ is available.
    \item Each computation of $J_N(\theta)$ requires an entire simulation of the model from $t=1$ to $t=N$.
    \item Usually $J_N(\theta)$ is a non-quadratic and non-convex function. Iterative and randomized optimization methods must be used.
    \item It's intuitive but very computationally demanding.
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm,auto,>=latex']
        \draw (0,2) rectangle ++(2,2);
        \node[align=center] at (1,3) {Model of\\the system};
        \node[draw, ellipse, align=center] at (1,0) (m) {Model\\$\mathcal{M}(\theta)$};
        \node[sum] at (4,0) (sum) {};
        \node[block] at (4,-1.5) (J) {$J(\theta)$};

        \draw[<-] (m) -- (-1,0) node[left] {$\tilde{F}(t)$};
        \draw[->] (m) -- (sum) node[pos=0.8] {+} node[pos=0.5] {$\hat{y}(t)$};
        \draw[->] (4,3) -- (sum) node[pos=0.9] {-};
        \draw[->] (sum) -- (J) node[pos=0.5, right] {\footnotesize simulation error};
        \draw[->] (J) -| (m);

        \draw[<-] (0,3) -- (-1,3) node[left] {$\tilde{F}(t)$};
        \draw[->] (2,3) -- (5,3) node[right] {$\tilde{y}(t)$};
    \end{tikzpicture}
\end{figure}

Can S.E.M. be applied also to B.B. methods?

\begin{example}
    We collect data $\{ \tilde{u}(1), \tilde{u}(2), \dots, \tilde{u}(N) \}$ and $\{ \tilde{y}(1), \tilde{y}(2), \dots, \tilde{y}(N) \}$, we want to estimate from data the I/O model.

    \[
        y(t) = \frac{b_0 + b_1z^{-1}}{1+a_1z^{-1} + a_2z^{-2}}u(t-1) \qquad \theta = \begin{bmatrix}
            a_1 \\ a_2 \\ b_0 \\ b_1
        \end{bmatrix}
    \]

    In time domain $y(t) = -a_1y(t-1)-a_2y(t-2)+b_0u(t-1)+b_1u(t-2)$.

    Using P.E.M.
    \[
        \hat{y}(t|t-1) = -a_1\hat{y}(t-1)-a_2\hat{y}(t-2)+b_0\hat{u}(t-1)+b_1\hat{u}(t-2)
    \]
    \begin{align*}
        J_N(\theta) &= \frac{1}{N}\sum_{t=1}^N \left( \tilde{y}(t) - \hat{y}(t|t-1, \theta) \right)^2 \\
        &= \frac{1}{N}\sum_{t=1}^N \left( \tilde{y}(t) +a_1\tilde{y}(t-1)+a_2\tilde{y}(t-2)-b_0\tilde{u}(t-1)-b_1\tilde{u}(t-2) \right)^2 \\
    \end{align*}

    Notice that it's a quadratic formula.

    \begin{figure}[H]
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \begin{tikzpicture}[node distance=2.5cm,auto,>=latex']
                \node[block] at (1,1) (zu) {$z^{-1}$};
                \node[block] at (1,3.5) (zy) {$z^{-1}$};

                \node at (0,2) (u) {$\tilde{u}(t)$};
                \node at (0,4.5) (y) {$\tilde{y}(t)$};

                \node[block,minimum height=5cm,minimum width=1.5cm,align=center] at (3.5,2.5) (sys) {Linear\\function\\of $\theta$};

                \draw[->] (zu) -- (zu-|sys.west) node[pos=0.5] {$\scriptstyle\tilde{u}(t-1)$};
                \draw[->] (u) -- (u-|sys.west);
                \draw[->] (1,2) -- (zu);
                \draw[->] (zy) -- (zy-|sys.west) node[pos=0.5] {$\scriptstyle\tilde{y}(t-1)$};
                \draw[->] (y) -- (y-|sys.west);
                \draw[->] (1,4.5) -- (zy);
                \draw[->] (sys) -- ++(2,0) node[above] {$\scriptstyle\hat{y}(t|t-1)$};
            \end{tikzpicture}
            \caption*{P.E.M.}
        \end{minipage}
        \begin{minipage}[t]{0.4\textwidth}
            \centering
            \begin{tikzpicture}[node distance=2.5cm,auto,>=latex']
                \node[block] at (1,1) (zu) {$z^{-1}$};
                \node[block] at (1,3) (zy) {$z^{-1}$};
                \node[block] at (1,4.5) (zy2) {$z^{-1}$};

                \node at (0,2) (u) {$\tilde{u}(t)$};

                \node[block,minimum height=5cm,minimum width=1.5cm,align=center] at (3.5,2.5) (sys) {Linear\\function\\of $\theta$};

                \draw[->] (zu) -- (zu-|sys.west) node[pos=0.5] {$\scriptstyle\tilde{u}(t-1)$};
                \draw[->] (u) -- (u-|sys.west);
                \draw[->] (1,2) -- (zu);

                \draw[->] (zy) -- (zy-|sys.west) node[pos=0.5] {$\scriptstyle\hat{y}(t-2)$};
                \draw[->] (zy2) -- (zy2-|sys.west) node[pos=0.5] {$\scriptstyle\hat{y}(t-1)$};
                \draw[->] (zy2) -- (zy);
                \draw[->] (4.8,2.5) -- (4.8,5.5) -- (1,5.5) -- (zy2);

                \draw[->] (sys) -- ++(2,0) node[above] {$\scriptstyle\hat{y}(t|t-1)$};
            \end{tikzpicture}
            \caption*{S.E.M.}
        \end{minipage}
    \end{figure}

    Using S.E.M.
    \[
        \hat{y}(t|t-1) = -a_1\hat{y}(t-1)-a_2\hat{y}(t-2)+b_0\tilde{u}(t-1)+b_1\tilde{u}(t-2)
    \]
    \begin{align*}
        J_N(\theta) &= \frac{1}{N}\sum_{t=1}^N \left( \tilde{y}(t) - \hat{y}(t|t-1, \theta) \right)^2 \\
        &= \frac{1}{N}\sum_{t=1}^N \left( \tilde{y}(t) +a_1\hat{y}(t-1)+a_2\hat{y}(t-2)-b_0\tilde{u}(t-1)-b_1\tilde{u}(t-2) \right)^2 \\
    \end{align*}

    Notice that it's non-linear with respect to $\theta$.
\end{example}

P.E.M. approach looks much better, but do not forget the noise! P.E.M. is much less robust w.r.t. noise, we must include a model of the noise in the estimated model.
We use ARMAX models.

If we use ARX models:
\[
    y(t) = \frac{b_0+b_1z^{-1}}{1+a_1z^{-1}+a_2z^{-2}}u(t-1) + \frac{1}{1+a_1z^{-1}+a_2z^{-2}}e(t)
\]
\[
    \hat{y}(t|t-1) = b_0u(t-1)+b_1u(t-2) - a_1y(t-1)-a_2y(t-2)
\]

If we use ARMAX models the numerator of the T.F. for $e(t)$ is $1+c_1z^{-1}+\ldots+c_mz^{-m}$, in this case $J_N(\theta)$ is non-linear.
This leads to the same complexity of S.E.M.

The second problem of P.E.M. is high sensitivity to sampling time choice.
Remember that when we write at discrete time $y(t)$ we mean $y(t\cdot \Delta)$.

\[
    \hat{y}(t|t-1) = -a_1\tilde{y}(t-1)-a_2\tilde{y}(t-2) + b_0\tilde{u}(t-1)+b_1\tilde{u}(t-2)
\]

If $\Delta$ is very small the difference between $\tilde{y}(t)$ and $\tilde{y}(t-1)$ becomes very small.
The effect is that the P.E.M. optimization ends to provide this \emph{trivial} solution:
\[
    a_1 = -1 \qquad a_2 \rightarrow 0 \qquad b_0 \rightarrow 0 \qquad b_1 \rightarrow 0 \qquad \Rightarrow \qquad \tilde{y}(t) \approx \tilde{y}(t-1)
\]

This is a wrong model due to the fact that the recursive part of the model is using past measures of the output instead of past values of the model outputs.

\section{Conclusion}

Summary of system identification methods for I/O systems
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm,auto,>=latex']
        \node[block, dashed border, minimum width=1.5cm, minimum height=1.5cm] at (0,0) (sys) {};
        \draw[<-] (sys) -- ++(-1.5,0) node[left] {$u(t)$};
        \draw[->] (sys) -- ++(1.5,0) node[right] {$y(t)$};
    \end{tikzpicture}
\end{figure}

\begin{itemize}
    \item Collect a dataset for training (if needed)
    \item Choose a model domain (linear static/non-linear static/linear dynamic/non-linear dynamic), using gray-box or black-box
    \item Estimation method: constructive (4SID), parametric (P.E.M. or S.E.M.) or filtering (state extension of K.F.)
\end{itemize}

Better black-box for system identification and software-sensing or white box?

It depends on the goals and type of applications.

\begin{itemize}
    \item Black box is very general and very flexible, make maximum use of data and no or little need of domain knowhow
    \item White box is very useful when you are the system-designer (not only the control algorithm designer), can provide more insight in the system.
    \item Gray box can sometimes be obtained by hybrid systems (part is black-box and part is white-box).
\end{itemize}